from pathlib import Path


def search_words():
    data_from_file_path = r"C:\Users\Kacper\pyth\e.txt"
    with open(data_from_file_path, "r",
              encoding="utf-8") as data_from_file:  # After line 9, the .txt file will be automatically closed
        # We do not have to take care of this in further code, especially where the function declaration finishes
        readable_data = data_from_file.read()
    return tokenizacja(readable_data)


def tokenizacja(readable_data): #Delete spaces and divide sentence into list of words
    splitted = readable_data.split(" ")
    return splitted


def load_dict():
    swapped_dict = {
        "i": "oraz",
        "oraz": "i",
        "nigdy": "prawie nigdy",
        "dlaczego": "czemu"
    }
    return swapped_dict


def swap_words(splitted, swapped_dict):
    split_len = len(splitted)
    for number in range(0, split_len): #Using words in increasing order, through splitted list
            if splitted[number] in swapped_dict.keys(): #If the word from splitted is included in dictionary...
                splitted[number] = swapped_dict[splitted[number]] #then change it with dictionary key for this word
    return splitted


def reverse_token(splitted): #Add spaces which were deleted for tokening purposes
    return " ".join(splitted)


def save_to_file(splitted):
    with open(r"C:\Users\Kacper\pyth\ss.txt", "w",
              encoding="utf-8") as data_from_file:  # After line 9, the .txt file will be automatically closed
        # We do not have to take care of this in further code, especially where the function declaration finishes
        data_from_file.write(splitted)


def main():
    splitted = search_words()
    dict = load_dict()
    del_w = swap_words(splitted, dict)
    text = reverse_token(del_w)
    save_to_file(text)


if __name__ == '__main__':
    main()
